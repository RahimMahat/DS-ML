{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fdf375",
   "metadata": {},
   "source": [
    "## MNIST handwritten digit recognition algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865d694",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc0f3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caae0c1",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d696d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading mnist dataset with info \n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# the tensorflow only provieds us the train and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# let's make a validation dataset\n",
    "                        # taking 10% of the train_dataset\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# write the function to scale the image\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa894f02",
   "metadata": {},
   "source": [
    "#### Shuffle and Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb56454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer_size = 1, no shuffling will actually happen\n",
    "# buffer_size >= num_samples, shuffling will ahppen ata once (uniformly)\n",
    "# if 1 < buffer_size < num_samples, we will be optimizing the computational power\n",
    "BUFFER_SIZE = 10_000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# batch size = 1 = Stocahstic gradient descent (SGD)\n",
    "# batch size = # samples = (single batch) GD\n",
    "# 1 < batch size < # samples = mini-batch GD\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115aa63",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014bf0f2",
   "metadata": {},
   "source": [
    "#### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fb95c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 784 inputs, and 10 known outputs, we will work with 2 hidden layers cosisting of 50 nodes each\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100 # as we were trying to get higher number of accuracy we tried fiddling around with the hidden_layer_size, started from 50\n",
    "        # function that is laying down the model (used to 'stack the layers')\n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Flatten(original shape) -> transform (flattens) a tensor into a vector\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    # tf.keras.layers.Dense(output size) -> taken the inputs, provides the model and calculates the dot product of the inputs and the weights and add the bias, This is also where we can apply an activation function\n",
    "    # our first hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    # and the second hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    # specifying the output layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e45ae",
   "metadata": {},
   "source": [
    "#### Select the Optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d5e9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer, loss, metrics) -> configures the model for training\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c08f4d",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2859b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 6s - loss: 0.3290 - accuracy: 0.9059 - val_loss: 0.1554 - val_accuracy: 0.9513\n",
      "Epoch 2/5\n",
      "540/540 - 4s - loss: 0.1340 - accuracy: 0.9603 - val_loss: 0.1292 - val_accuracy: 0.9587\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0965 - accuracy: 0.9708 - val_loss: 0.0940 - val_accuracy: 0.9717\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0741 - accuracy: 0.9771 - val_loss: 0.0723 - val_accuracy: 0.9773\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0590 - accuracy: 0.9826 - val_loss: 0.0649 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f11c3cf2b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "# fit the model\n",
    "model.fit(train_data, epochs= NUM_EPOCHS, validation_data= (validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878d9d7",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2253239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0848 - accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "# model.evaluate() -> returns the loss value and metrics values for the model in 'test mode'\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56782a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.22%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss: {test_loss:.2f}. Test accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34753166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have successfully trained and tested our machine learning algorithm with 97% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
